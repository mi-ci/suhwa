{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1x5cRykShWW_KHBpjye8zHPnPFNbq1YC7","timestamp":1713234388170},{"file_id":"1SVqtMoB0HBCS6OuDo8877cWMIgZyAdy8","timestamp":1713230549759},{"file_id":"1QsHKerYWVInq5Ur-dZB16yGJ0id5OYep","timestamp":1682008737137},{"file_id":"https://github.com/googlesamples/mediapipe/blob/main/codelabs/gesture_recognizer/WiMLS_2022_MediaPipe_Gesture_Recognizer_Walkthrough.ipynb","timestamp":1676328042906}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["##### Copyright 2023 The MediaPipe Authors. All Rights Reserved."],"metadata":{"id":"QRXrUrT7vpJm"}},{"cell_type":"code","source":["!pip install pip==21.3.1"],"metadata":{"id":"F_XEkrJ7poVA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1713237650189,"user_tz":-540,"elapsed":9376,"user":{"displayName":"neve J","userId":"07819571134254880107"}},"outputId":"797fad8b-a098-40f9-d1c5-868311a49d99"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pip==21.3.1\n","  Downloading pip-21.3.1-py3-none-any.whl (1.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pip\n","  Attempting uninstall: pip\n","    Found existing installation: pip 23.1.2\n","    Uninstalling pip-23.1.2:\n","      Successfully uninstalled pip-23.1.2\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","pip-tools 6.13.0 requires pip>=22.2, but you have pip 21.3.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed pip-21.3.1\n"]}]},{"cell_type":"code","source":["import cv2"],"metadata":{"id":"kKCar6AhDuwO","executionInfo":{"status":"ok","timestamp":1713237650713,"user_tz":-540,"elapsed":547,"user":{"displayName":"neve J","userId":"07819571134254880107"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["!unzip -qq label_images.zip"],"metadata":{"id":"g_4HuuVU3M7G","executionInfo":{"status":"ok","timestamp":1713237651022,"user_tz":-540,"elapsed":312,"user":{"displayName":"neve J","userId":"07819571134254880107"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["IMAGES_PATH = 'label_images'"],"metadata":{"id":"lEsU60kdtrjH","executionInfo":{"status":"ok","timestamp":1713237651022,"user_tz":-540,"elapsed":5,"user":{"displayName":"neve J","userId":"07819571134254880107"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["## Making a New Model\n","\n"],"metadata":{"id":"qmgcSMPeyk0P"}},{"cell_type":"code","source":["!pip install -q mediapipe-model-maker"],"metadata":{"id":"gxKdCdj2y_IM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1713237911410,"user_tz":-540,"elapsed":138830,"user":{"displayName":"neve J","userId":"07819571134254880107"}},"outputId":"77ba0dfd-67d7-4585-ba20-59532b749fd7"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["     |████████████████████████████████| 127 kB 3.1 MB/s            \n","     |████████████████████████████████| 611 kB 6.5 MB/s            \n","     |████████████████████████████████| 35.6 MB 488 kB/s            \n","     |████████████████████████████████| 2.7 MB 78.4 MB/s            \n","     |████████████████████████████████| 5.2 MB 74.5 MB/s            \n","     |████████████████████████████████| 43 kB 2.2 MB/s             \n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","     |████████████████████████████████| 106 kB 66.6 MB/s            \n","     |████████████████████████████████| 1.7 MB 52.8 MB/s            \n","     |████████████████████████████████| 242 kB 68.8 MB/s            \n","     |████████████████████████████████| 589.8 MB 13 kB/s             \n","     |████████████████████████████████| 2.7 MB 63.1 MB/s            \n","     |████████████████████████████████| 5.2 MB 69.6 MB/s            \n","\u001b[?25h  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"]}]},{"cell_type":"code","source":["from mediapipe_model_maker import gesture_recognizer"],"metadata":{"id":"GTCjgBETDqU0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load the rock-paper-scissor image archive.\n","data = gesture_recognizer.Dataset.from_folder(\n","    dirname=IMAGES_PATH,\n","    hparams=gesture_recognizer.HandDataPreprocessingParams()\n",")\n","\n","# Split the archive into training, validation and test dataset.\n","train_data, rest_data = data.split(0.8)\n","validation_data, test_data = rest_data.split(0.5)"],"metadata":{"id":"Xh33Bg6QC5cz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1713238029880,"user_tz":-540,"elapsed":9284,"user":{"displayName":"neve J","userId":"07819571134254880107"}},"outputId":"e446aa6c-f509-42b9-9f70-5e42c9d79aeb"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading https://storage.googleapis.com/mediapipe-assets/palm_detection_full.tflite to /tmp/model_maker/gesture_recognizer/palm_detection_full.tflite\n","Downloading https://storage.googleapis.com/mediapipe-assets/hand_landmark_full.tflite to /tmp/model_maker/gesture_recognizer/hand_landmark_full.tflite\n","Downloading https://storage.googleapis.com/mediapipe-assets/gesture_embedder.tar.gz to /tmp/model_maker/gesture_recognizer/gesture_embedder\n"]}]},{"cell_type":"code","source":["# Train the model\n","hparams = gesture_recognizer.HParams(export_dir=\"suhwa_model\")\n","options = gesture_recognizer.GestureRecognizerOptions(hparams=hparams)\n","model = gesture_recognizer.GestureRecognizer.create(\n","    train_data=train_data,\n","    validation_data=validation_data,\n","    options=options\n",")"],"metadata":{"id":"YE-CyQglHQD1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1713238075661,"user_tz":-540,"elapsed":8026,"user":{"displayName":"neve J","userId":"07819571134254880107"}},"outputId":"e8a93572-c59d-43b5-e4c6-75f84ce1b251"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," hand_embedding (InputLayer  [(None, 128)]             0         \n"," )                                                               \n","                                                                 \n"," batch_normalization (Batch  (None, 128)               512       \n"," Normalization)                                                  \n","                                                                 \n"," re_lu (ReLU)                (None, 128)               0         \n","                                                                 \n"," dropout (Dropout)           (None, 128)               0         \n","                                                                 \n"," custom_gesture_recognizer_  (None, 8)                 1032      \n"," out (Dense)                                                     \n","                                                                 \n","=================================================================\n","Total params: 1544 (6.03 KB)\n","Trainable params: 1288 (5.03 KB)\n","Non-trainable params: 256 (1.00 KB)\n","_________________________________________________________________\n","None\n","Epoch 1/10\n","19/19 [==============================] - 2s 40ms/step - loss: 2.3985 - categorical_accuracy: 0.0000e+00 - val_loss: 0.9456 - val_categorical_accuracy: 0.4000 - lr: 0.0010\n","Epoch 2/10\n","19/19 [==============================] - 0s 24ms/step - loss: 1.7054 - categorical_accuracy: 0.1053 - val_loss: 0.6595 - val_categorical_accuracy: 0.4000 - lr: 9.9000e-04\n","Epoch 3/10\n","19/19 [==============================] - 0s 21ms/step - loss: 1.3283 - categorical_accuracy: 0.2368 - val_loss: 0.6361 - val_categorical_accuracy: 0.4000 - lr: 9.8010e-04\n","Epoch 4/10\n","19/19 [==============================] - 0s 20ms/step - loss: 1.0893 - categorical_accuracy: 0.3421 - val_loss: 0.6148 - val_categorical_accuracy: 0.4000 - lr: 9.7030e-04\n","Epoch 5/10\n","19/19 [==============================] - 0s 21ms/step - loss: 1.0131 - categorical_accuracy: 0.3158 - val_loss: 0.5558 - val_categorical_accuracy: 0.6000 - lr: 9.6060e-04\n","Epoch 6/10\n","19/19 [==============================] - 0s 21ms/step - loss: 0.8211 - categorical_accuracy: 0.5263 - val_loss: 0.5318 - val_categorical_accuracy: 0.6000 - lr: 9.5099e-04\n","Epoch 7/10\n","19/19 [==============================] - 0s 20ms/step - loss: 0.7515 - categorical_accuracy: 0.5526 - val_loss: 0.5338 - val_categorical_accuracy: 0.6000 - lr: 9.4148e-04\n","Epoch 8/10\n","19/19 [==============================] - 0s 23ms/step - loss: 0.6756 - categorical_accuracy: 0.5526 - val_loss: 0.5251 - val_categorical_accuracy: 0.6000 - lr: 9.3207e-04\n","Epoch 9/10\n","19/19 [==============================] - 0s 20ms/step - loss: 0.6247 - categorical_accuracy: 0.6579 - val_loss: 0.5385 - val_categorical_accuracy: 0.6000 - lr: 9.2274e-04\n","Epoch 10/10\n","19/19 [==============================] - 0s 21ms/step - loss: 0.5845 - categorical_accuracy: 0.6316 - val_loss: 0.5294 - val_categorical_accuracy: 0.6000 - lr: 9.1352e-04\n"]}]},{"cell_type":"code","source":["loss, acc = model.evaluate(test_data, batch_size=1)\n","print(f\"Test loss:{loss}, Test accuracy:{acc}\")"],"metadata":{"id":"ULANbzL7HnIR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1713238081105,"user_tz":-540,"elapsed":546,"user":{"displayName":"neve J","userId":"07819571134254880107"}},"outputId":"4e1fa941-a208-4f80-d385-9235b114a83b"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["5/5 [==============================] - 0s 6ms/step - loss: 0.5959 - categorical_accuracy: 0.6000\n","Test loss:0.5958923697471619, Test accuracy:0.6000000238418579\n"]}]},{"cell_type":"code","source":["# Export the model bundle.\n","model.export_model()\n","\n","# Rename the file to be more descriptive.\n","!mv suhwa_model/gesture_recognizer.task suhwas.task"],"metadata":{"id":"aMZQc767I9Q-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1713238158072,"user_tz":-540,"elapsed":1416,"user":{"displayName":"neve J","userId":"07819571134254880107"}},"outputId":"20c9cd50-c414-4cab-86f7-4f692acdf3e5"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Using existing files at /tmp/model_maker/gesture_recognizer/gesture_embedder.tflite\n","Using existing files at /tmp/model_maker/gesture_recognizer/palm_detection_full.tflite\n","Using existing files at /tmp/model_maker/gesture_recognizer/hand_landmark_full.tflite\n","Using existing files at /tmp/model_maker/gesture_recognizer/canned_gesture_classifier.tflite\n"]}]},{"cell_type":"code","source":["img = cv2.imread(\"test.jpg\")\n","cv2_imshow(img)"],"metadata":{"id":"7fPaL2Uv07l2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Imports neccessary modules.\n","import mediapipe as mp\n","from mediapipe.tasks import python\n","from mediapipe.tasks.python import vision\n","import os\n","\n","# Create a GestureRecognizer object.\n","model_path = os.path.abspath(\"suhwas.task\")\n","recognizer = vision.GestureRecognizer.create_from_model_path(model_path)\n","\n","# Load the input image.\n","image = mp.Image.create_from_file('test.jpg')\n","\n","# Run gesture recognition.\n","recognition_result = recognizer.recognize(image)\n","\n","# Display the most likely gesture.\n","top_gesture = recognition_result.gestures[0][0]\n","print(f\"Gesture recognized: {top_gesture.category_name} ({top_gesture.score})\")"],"metadata":{"id":"iiXfexYg0fph","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1713238416865,"user_tz":-540,"elapsed":352,"user":{"displayName":"neve J","userId":"07819571134254880107"}},"outputId":"b4a1daac-2ee0-4aec-9441-99d5df35801a"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Gesture recognized:  (0.5353529453277588)\n"]}]}]}